{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US_array\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import datetime\n",
    "import random\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import netCDF4\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.colors as mcolors\n",
    "import h5py\n",
    "import argparse\n",
    "import pymap3d as pm\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import copy\n",
    "import re\n",
    "import matplotlib.patches as patches\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "from scipy import signal\n",
    "from scipy import spatial\n",
    "from torch.nn import Linear\n",
    "from torch import Tensor\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data.sampler import SubsetRandomSampler,WeightedRandomSampler\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from pyproj import Proj\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from glob import glob\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes,mark_inset\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "# for reproducibility\n",
    "os.environ['PYTHONHASHSEED']= '123'\n",
    "np.random.seed(123)\n",
    "\n",
    "# use custom Matplotlib style\n",
    "plt.style.use(\"./science.mplstyle\")\n",
    "\n",
    "# loading data\n",
    "path_file = '../../00_data/glad-m25-vp-0.0-n4.nc'\n",
    "\n",
    "data = xr.open_dataset(path_file)\n",
    "\n",
    "offline = False\n",
    "old = False\n",
    "\n",
    "if offline and old:\n",
    "    dep_ini, dep_inc = 11, 4\n",
    "    lat_ini, lat_inc = 0, 8\n",
    "    lon_ini, lon_inc = 0, 8\n",
    "else:\n",
    "    dep_ini, dep_inc = 11, 1\n",
    "    lat_ini, lat_inc = 0, 2\n",
    "    lon_ini, lon_inc = 0, 2    \n",
    "\n",
    "# variables\n",
    "vpv = data.variables['vpv'].values[dep_ini::dep_inc, lat_ini::lat_inc, lon_ini::lon_inc]\n",
    "vph = data.variables['vph'].values[dep_ini::dep_inc, lat_ini::lat_inc, lon_ini::lon_inc]\n",
    "longitude = data.variables['longitude'].values[lon_ini::lon_inc]\n",
    "latitude = data.variables['latitude'].values[lat_ini::lat_inc]\n",
    "depth = data.variables['depth'].values[dep_ini::dep_inc]\n",
    "\n",
    "dep_dim = vpv.shape[0]\n",
    "lat_dim = vpv.shape[1]\n",
    "lon_dim = vpv.shape[2]\n",
    "\n",
    "# geodetic-geocentric projection\n",
    "LAT, ALT, LON = np.meshgrid(latitude, -1e3*depth, longitude)\n",
    "x, y, z = pm.geodetic2ecef(LAT, LON, ALT)\n",
    "_, DEP, _ = np.meshgrid(latitude, depth, longitude)\n",
    "\n",
    "# multi-receivers options\n",
    "rec_typ = 'US_array'\n",
    "\n",
    "# type of \"play ground\"\n",
    "experiment = \"mean_receiver_velocity_all_points\"\n",
    "\n",
    "# saving parameters\n",
    "num_pts = x.size\n",
    "num_epo = int(2001)\n",
    "num_blo = 21 #20\n",
    "coo_sys = 'cartesian'\n",
    "vel_sha = 'sphere'\n",
    "vel_typ = 'gladm25'\n",
    "num_neu = 512\n",
    "lea_rat = 5e-6\n",
    "act_fun = torch.nn.ELU\n",
    "bat_siz = 512 #num_pts // 100\n",
    "ada_wei = False\n",
    "opt_fun = torch.optim.Adam\n",
    "dev_typ = \"cuda\"\n",
    "bac_vel = 10 #6 #10.5\n",
    "hyp_par = (\n",
    "    str(num_epo) + '_' +\n",
    "    str(num_blo) + '_' +\n",
    "    str(lea_rat) + '_' +\n",
    "    str(dep_dim) + '_' +\n",
    "    str(lat_dim) + '_' +\n",
    "    str(lon_dim) + '_' +\n",
    "    str(act_fun) + '_' +\n",
    "    str(ada_wei) + '_' +\n",
    "    str(num_pts) + '_' +\n",
    "    str(bat_siz) + '_' +\n",
    "    vel_sha + '_' +\n",
    "    vel_typ + '_' +\n",
    "    str(num_neu) + '_' +\n",
    "    coo_sys + '_' +\n",
    "    str(opt_fun) + '_' +\n",
    "    str(bac_vel) + '_' +\n",
    "    rec_typ + '_' +\n",
    "    dev_typ + '_' +\n",
    "    experiment\n",
    ")\n",
    "\n",
    "# # path\n",
    "# model_path = \"./../results/\" + hyp_par\n",
    "# figures_path = model_path + '/'\n",
    "# checkpoints_path = figures_path + 'checkpoints' + '/'\n",
    "# predictions_path = figures_path + 'predictions' + '/'\n",
    "\n",
    "# Path(figures_path).mkdir(parents=True, exist_ok=True)\n",
    "# Path(checkpoints_path).mkdir(parents=True, exist_ok=True)\n",
    "# Path(predictions_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# training points\n",
    "# laod all stations\n",
    "ISCall = pd.read_csv('stations.csv')\n",
    "ISCall = ISCall.rename(columns={\"X\":\"LON\", 'Y':'LAT'})\n",
    "\n",
    "# laod only active stations (from ISC website) to 2021\n",
    "ISCarray = ISCall[ISCall['description'].str.contains('to 2021')]\n",
    "ISCarrLon = ISCarray['LON']\n",
    "ISCarrLat = ISCarray['LAT']\n",
    "\n",
    "# load US array data\n",
    "USarray = pd.read_excel('_US-TA-StationList.xls')\n",
    "USarrLon = USarray['LON']\n",
    "USarrLat = USarray['LAT']\n",
    "\n",
    "# concatenate the two receiver group\n",
    "AllLon = np.hstack((USarrLon, ISCarrLon))\n",
    "AllLat = np.hstack((USarrLat, ISCarrLat))\n",
    "\n",
    "# model specifications\n",
    "ear_rad = 6371\n",
    "\n",
    "# source vectors\n",
    "if rec_typ == 'ISC_array':\n",
    "    print(rec_typ)\n",
    "    lat_sou = np.array([latitude.flat[np.abs(latitude - i).argmin()] for i in ISCarrLat])\n",
    "    lon_sou = np.array([longitude.flat[np.abs(longitude - i).argmin()] for i in ISCarrLon])\n",
    "elif rec_typ == 'US_array':\n",
    "    print(rec_typ)\n",
    "    lat_sou = np.array([latitude.flat[np.abs(latitude - i).argmin()] for i in USarrLat])\n",
    "    lon_sou = np.array([longitude.flat[np.abs(longitude - i).argmin()] for i in USarrLon])\n",
    "dep_sou = depth.flat[np.abs(depth - 0).argmin()]\n",
    "\n",
    "xx = (ear_rad - DEP) * np.sin(np.radians(LAT+90)) * np.cos(np.radians(180+LON))/(1e3)\n",
    "yy = (ear_rad - DEP) * np.sin(np.radians(LAT+90)) * np.sin(np.radians(180+LON))/(1e3)\n",
    "zz = DEP * np.cos(np.radians(LAT+90)) / (1e3)\n",
    "\n",
    "xx_s = (ear_rad - dep_sou) * np.sin(np.radians(lat_sou+90)) * np.cos(np.radians(180+lon_sou))/(1e3)\n",
    "yy_s = (ear_rad - dep_sou) * np.sin(np.radians(lat_sou+90)) * np.sin(np.radians(180+lon_sou))/(1e3)\n",
    "\n",
    "# coordinates setup\n",
    "sx, sy, sz = pm.geodetic2ecef(lat_sou, lon_sou, -1e3*dep_sou)\n",
    "\n",
    "# rescale\n",
    "X,Y,Z = x/(ear_rad*1e3), y/(ear_rad*1e3), z/(ear_rad*1e3)\n",
    "sx, sy, sz = sx/(ear_rad*1e3), sy/(ear_rad*1e3), sz/(ear_rad*1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15139509.7, 16777216)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.7*X.size,  2**24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding source indices using Dask\n",
    "import dask.array as da\n",
    "sids = da.where((da.isclose(da.from_array(X.reshape(-1,1), chunks=X.size//100), da.from_array(sx), atol=1e-16)) & (da.isclose(da.from_array(Y.reshape(-1,1), chunks=X.size//100), da.from_array(sy), atol=1e-16)) & (da.isclose(da.from_array(Z.reshape(-1,1), chunks=X.size//100), da.from_array(sz), atol=1e-16)))[0]\n",
    "sx, sy, sz = X.reshape(-1,1)[sids], Y.reshape(-1,1)[sids], Z.reshape(-1,1)[sids]\n",
    "sids = sids[sids.astype(bool)].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding source indices using Dask\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.from_array(X.reshape(-1,1), chunks=X.size//100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.from_array(X.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d28a8c50980d99a58a412893c2eef7e0fc9e924de74c9e494ef03e23cc25d15"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('thesis-pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
