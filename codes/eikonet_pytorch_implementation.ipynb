{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# modules_import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import datetime\n",
    "import random\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import netCDF4\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.colors as mcolors\n",
    "import h5py\n",
    "import argparse\n",
    "import pymap3d as pm\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "from scipy import signal\n",
    "from torch.nn import Linear\n",
    "from torch import Tensor\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data.sampler import SubsetRandomSampler,WeightedRandomSampler\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from pyproj import Proj\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from glob import glob\n",
    "\n",
    "# for reproducibility\n",
    "os.environ['PYTHONHASHSEED']= '123'\n",
    "np.random.seed(123)\n",
    "\n",
    "params = {\n",
    "    'xtick.labelsize':'xx-small',\n",
    "    'ytick.labelsize':'xx-small',\n",
    "    'figure.dpi':300\n",
    "}\n",
    "pylab.rcParams.update(params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# model_parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## V(x,y,z)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# model specifications\n",
    "ear_rad = 6371\n",
    "v0 = 5\n",
    "zmin = 0.; zmax = 2.; deltaz = 0.05;\n",
    "xmin = 0.; xmax = 2.; deltax = 0.05;\n",
    "ymin = 0.; ymax = 2.; deltay = 0.05;\n",
    "\n",
    "# coordinates setup\n",
    "z = np.arange(zmin,zmax+deltaz,deltaz)\n",
    "nz = z.size\n",
    "y = np.arange(ymin,ymax+deltay,deltay)\n",
    "ny = y.size\n",
    "x = np.arange(xmin,xmax+deltax,deltax)\n",
    "nx = x.size\n",
    "X,Y,Z = np.meshgrid(x,y,z)\n",
    "\n",
    "# point-source location\n",
    "sz = 1.; sx = 1.; sy=0.;\n",
    "sx = x.flat[np.abs(x - sx).argmin()]\n",
    "sy = y.flat[np.abs(y - sy).argmin()]\n",
    "sz = z.flat[np.abs(z - sz).argmin()]\n",
    "print(sx,sy,sz)\n",
    "\n",
    "# hyperparameters\n",
    "num_lay = 128*128\n",
    "num_epo = int(2e3)\n",
    "lea_rat = 5e-3\n",
    "act_fun = 'relu'\n",
    "num_pts = x.size\n",
    "bat_siz = 4000\n",
    "vel_typ = 'inhomogeneous'\n",
    "vel_sha = 'sphere'\n",
    "coo_sys = 'cartesian'\n",
    "ker_ini = 'glorot_uniform'\n",
    "res_net = True\n",
    "opt_fun = 'adam'\n",
    "\n",
    "# preparing velocity model\n",
    "dv_dz = 0.5; dv_dx = 0.; dv_dy = 0.; \n",
    "vel_sou = 2. + dv_dz*sz + dv_dx*sx + dv_dy*sy\n",
    "vel_mod = vel_sou + dv_dz*(Z-sz) + dv_dx*(X-sx) + dv_dy*(Y-sy)\n",
    "\n",
    "# traveltime solution\n",
    "if dv_dz==0 and dv_dx==0: \n",
    "    \n",
    "  # for homogeneous velocity model\n",
    "  T_data = np.sqrt((Z-sz)**2 + (Y-sy)**2 + (X-sx)**2)/2.\n",
    "else: \n",
    "    \n",
    "  # for velocity gradient model\n",
    "  T_data = np.arccosh(1.0+0.5*(1.0/vel_mod)*(1/vel_sou)*(dv_dz**2 + dv_dy**2 + dv_dx**2)*((X-sx)**2 + (Y-sy)**2 + (Z-sz)**2))/np.sqrt(dv_dz**2 + dv_dy**2 + dv_dx**2)\n",
    "\n",
    "# grid points for prediction \n",
    "X_star = [X.reshape(-1,1), ] \n",
    "\n",
    "# velocity at the source location\n",
    "vel = vel_mod[int(round(sz/deltaz)),int(round(sx/deltax))]\n",
    "\n",
    "# for plotting only\n",
    "x_plot,y_plot,z_plot = X*ear_rad/1000,Y*ear_rad/1000,Z*ear_rad/1000"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0 0.0 1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## V(rad,the,phi)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import\n",
    "from projection import *\n",
    "\n",
    "# model specifications\n",
    "ear_rad = 6371\n",
    "lat_sou = latitude.flat[np.abs(latitude - -8).argmin()]\n",
    "lon_sou = longitude.flat[np.abs(longitude - 110).argmin()]\n",
    "dep_sou = depth.flat[np.abs(depth - 1750).argmin()]\n",
    "\n",
    "# coordinates setup\n",
    "sx, sy, sz = pm.geodetic2ecef(lat_sou, lon_sou, -1e3*dep_sou)\n",
    "\n",
    "# rescale\n",
    "x,y,z = x/(ear_rad*1e3), y/(ear_rad*1e3), z/(ear_rad*1e3)\n",
    "sx, sy, sz = sx/(ear_rad*1e3), sy/(ear_rad*1e3), sz/(ear_rad*1e3)\n",
    "\n",
    "X,Y,Z = x,y,z\n",
    "\n",
    "print(np.where((np.isclose(x.reshape(-1,1), sx)) & (np.isclose(y.reshape(-1,1), sy)) & (np.isclose(z.reshape(-1,1), sz))))\n",
    "print(sx,sy,sz)\n",
    "\n",
    "# for plotting only\n",
    "x_plot,y_plot,z_plot = x.reshape(-1,1)*ear_rad/1000, y.reshape(-1,1)*ear_rad/1000, z.reshape(-1,1)*ear_rad/1000\n",
    "\n",
    "# saving parameters\n",
    "num_lay = 128*64\n",
    "num_epo = int(2e3)\n",
    "lea_rat = 1e-4\n",
    "act_fun = 'tanh'\n",
    "num_pts = x.size\n",
    "bat_siz = num_pts//500\n",
    "vel_typ = 'radial'\n",
    "vel_sha = 'sphere'\n",
    "coo_sys = 'cartesian'\n",
    "ker_ini = 'glorot_uniform'\n",
    "opt_fun = 'adam'\n",
    "hyp_par = (\n",
    "    str(num_lay) + '_' +\n",
    "    str(num_epo) + '_' +\n",
    "    str(lea_rat) + '_' +\n",
    "    str(dep_dim) + '_' +\n",
    "    str(lat_dim) + '_' +\n",
    "    str(lon_dim) + '_' +\n",
    "    act_fun + '_' +\n",
    "    str(num_pts) + '_' +\n",
    "    str(bat_siz) + '_' +\n",
    "    vel_typ + '_' +\n",
    "    vel_sha + '_' +\n",
    "    coo_sys + '_' +\n",
    "    ker_ini + '_' +\n",
    "    opt_fun\n",
    ")\n",
    "\n",
    "# path\n",
    "model_path = \"./\" + hyp_par\n",
    "figures_path = model_path + '/'\n",
    "checkpoints_path = figures_path + 'checkpoints' + '/'\n",
    "\n",
    "Path(figures_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(checkpoints_path).mkdir(parents=True, exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# def init_weights(m):\n",
    "#     if type(m) == torch.nn.Linear:\n",
    "#         stdv = (1. / math.sqrt(m.weight.size(1))/1.)*2\n",
    "#         m.weight.data.uniform_(-stdv,stdv)\n",
    "#         m.bias.data.uniform_(-stdv,stdv)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_normal(m.weight)\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, nl=10, activation=torch.nn.ELU()):\n",
    "            super(NN, self).__init__()\n",
    "            self.act = activation\n",
    "\n",
    "            # Input Structure\n",
    "            self.fc0  = Linear(2*3,32)\n",
    "            self.fc1  = Linear(32,512)\n",
    "\n",
    "            # Resnet Block\n",
    "            self.rn_fc1 = torch.nn.ModuleList([Linear(512, 512) for i in range(nl)])\n",
    "            self.rn_fc2 = torch.nn.ModuleList([Linear(512, 512) for i in range(nl)])\n",
    "            self.rn_fc3 = torch.nn.ModuleList([Linear(512, 512) for i in range(nl)])\n",
    "\n",
    "            # Output structure\n",
    "            self.fc8  = Linear(512,32)\n",
    "            self.fc9  = Linear(32,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x   = self.act(self.fc0(x))\n",
    "        x   = self.act(self.fc1(x))\n",
    "        for ii in range(len(self.rn_fc1)):\n",
    "            x0 = x\n",
    "            x  = self.act(self.rn_fc1[ii](x))\n",
    "            x  = self.act(self.rn_fc3[ii](x)+self.rn_fc2[ii](x0))\n",
    "\n",
    "        x     = self.act(self.fc8(x))\n",
    "        tau   = abs(self.fc9(x))\n",
    "        return tau\n",
    "\n",
    "# class NN(torch.nn.Module):\n",
    "#     def __init__(self, nl=128, activation=torch.nn.ELU()):\n",
    "#             super(NN, self).__init__()\n",
    "#             self.act = activation\n",
    "\n",
    "#             # Input Structure\n",
    "#             self.fc0  = Linear(2*3,128)\n",
    "\n",
    "#             # Hidden Layer\n",
    "#             self.fc1  = torch.nn.ModuleList([Linear(128, 128) for i in range(nl)])\n",
    "\n",
    "#             # Output structure\n",
    "#             self.fc9  = Linear(128,1)\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         x   = self.act(self.fc0(x))\n",
    "\n",
    "#         for ii in range(len(self.fc1)):\n",
    "#             x  = self.act(self.fc1[ii](x))\n",
    "\n",
    "#         tau   = abs(self.fc9(x))\n",
    "#         return tau\n",
    "\n",
    "def EikonalLoss(Yobs,Xp,tau,device):\n",
    "        dtau  = torch.autograd.grad(outputs=tau, inputs=Xp, grad_outputs=torch.ones(tau.size()).to(device), only_inputs=True,create_graph=True,retain_graph=True)[0]\n",
    "\n",
    "        T0    = torch.sqrt(((Xp[:,3]-Xp[:,0])**2 + (Xp[:,4]-Xp[:,1])**2 + (Xp[:,5]-Xp[:,2])**2))  \n",
    "        T1    = (T0**2)*(dtau[:,3]**2 + dtau[:,4]**2 + dtau[:,5]**2)\n",
    "        T2    = 2*tau[:,0]*(dtau[:,3]*(Xp[:,3]-Xp[:,0]) + dtau[:,4]*(Xp[:,4]-Xp[:,1]) + dtau[:,5]*(Xp[:,5]-Xp[:,2]))\n",
    "        T3    = tau[:,0]**2\n",
    "        S2    = (T1+T2+T3)\n",
    "        Ypred = torch.sqrt(1/S2)\n",
    "        diff  = abs(Yobs[:,1]-Ypred)/Yobs[:,1]\n",
    "        loss  = torch.mean(abs((Yobs[:,1]-Ypred)/Yobs[:,1]))\n",
    "        return loss, diff\n",
    "\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, ModelPath, VelocityClass, device='gpu'):\n",
    "        \n",
    "        self.Params = {}\n",
    "        self.Params['ModelPath']     = ModelPath\n",
    "        self.Params['VelocityClass'] = VelocityClass #Pass the JSON information\n",
    "        self.Params['Device']        = device\n",
    "        self.Params['Pytorch Amp (bool)'] = False\n",
    "\n",
    "        self.Params['Network']  = {}\n",
    "        self.Params['Network']['Number of Residual Blocks']  = 20\n",
    "        self.Params['Network']['Layer activation']           = torch.nn.ELU()\n",
    "        self.Params['Network']['Normlisation']               = 'MinMax'\n",
    "\n",
    "        self.Params['Training'] = {}\n",
    "        # self.Params['Training']['Number of sample points']   = 1e4\n",
    "        self.Params['Training']['Batch Size']                = 64\n",
    "        self.Params['Training']['Validation Percentage']     = 10\n",
    "        self.Params['Training']['Number of Epochs']          = 200\n",
    "        self.Params['Training']['Resampling Bounds']         = [0.1,0.9]\n",
    "        self.Params['Training']['Print Every * Epoch']       = 1\n",
    "        self.Params['Training']['Save Every * Epoch']        = 50\n",
    "        self.Params['Training']['Learning Rate']             = 5e-5\n",
    "        self.Params['Training']['Random Distance Sampling']  = False\n",
    "        self.Params['Training']['Use Scheduler (bool)']      = True\n",
    "\n",
    "        # Parameters to alter during training\n",
    "        self.total_train_loss = []\n",
    "        self.total_val_loss   = []\n",
    "\n",
    "\n",
    "    def _init_network(self):\n",
    "        self.network = NN(nl=self.Params['Network']['Number of Residual Blocks'],activation=self.Params['Network']['Layer activation'])\n",
    "        self.network.apply(init_weights)\n",
    "        self.network.float()\n",
    "        self.network.to(torch.device(self.Params['Device']))\n",
    "\n",
    "\n",
    "\n",
    "    def _projection(self,Xp,inverse=False):\n",
    "        if type(self.Params['VelocityClass'].projection) != type(None):\n",
    "            proj = Proj(self.Params['VelocityClass'].projection)\n",
    "            Xp = Xp.detach().cpu().numpy()\n",
    "            Xp[:,0],Xp[:,1] = proj(Xp[:,0],Xp[:,1],inverse=inverse)\n",
    "            Xp[:,3],Xp[:,4] = proj(Xp[:,3],Xp[:,4],inverse=inverse)\n",
    "            Xp = torch.Tensor(Xp)\n",
    "            Xp = Xp.to(torch.device(self.Params['Device']))\n",
    "        return Xp\n",
    "\n",
    "\n",
    "    def  _normalization(self,Xp=None,Yp=None):\n",
    "\n",
    "        # Loading the predefined variables\n",
    "        if self.Params['Network']['Normlisation'] == 'MinMax':\n",
    "            xmin_UTM = np.array(copy.copy(self.Params['VelocityClass'].xmin))\n",
    "            xmax_UTM = np.array(copy.copy(self.Params['VelocityClass'].xmax))\n",
    "            if type(self.Params['VelocityClass'].projection) == str:\n",
    "                proj = Proj(self.Params['VelocityClass'].projection)\n",
    "                xmin_UTM[0],xmin_UTM[1] = proj(xmin_UTM[0],xmin_UTM[1]) \n",
    "                xmax_UTM[0],xmax_UTM[1] = proj(xmax_UTM[0],xmax_UTM[1]) \n",
    "            indx = np.argmax(xmax_UTM-xmin_UTM)\n",
    "            self.nf_max    = xmax_UTM[indx]\n",
    "            self.nf_min    = xmin_UTM[indx]\n",
    "            self.sf        = (self.nf_max-self.nf_min)\n",
    "\n",
    "            if (type(Xp)!=type(None)) and (type(Yp)==type(None)):\n",
    "                Xp  = Xp/self.sf\n",
    "                return Xp\n",
    "            if (type(Xp)==type(None)) and (type(Yp)!=type(None)):\n",
    "                Yp  = Yp*self.sf\n",
    "                return Yp\n",
    "            else:\n",
    "                Xp = Xp/self.sf\n",
    "                Yp = Yp/self.sf\n",
    "                return Xp,Yp\n",
    "\n",
    "        if self.Params['Network']['Normlisation'] == 'OffsetMinMax':\n",
    "            xmin_UTM = np.array(copy.copy(self.Params['VelocityClass'].xmin))\n",
    "            xmax_UTM = np.array(copy.copy(self.Params['VelocityClass'].xmax))\n",
    "            if type(self.Params['VelocityClass'].projection) == str:\n",
    "                proj = Proj(self.Params['VelocityClass'].projection)\n",
    "                xmin_UTM[0],xmin_UTM[1] = proj(xmin_UTM[0],xmin_UTM[1]) \n",
    "                xmax_UTM[0],xmax_UTM[1] = proj(xmax_UTM[0],xmax_UTM[1]) \n",
    "            indx = np.argmax(xmax_UTM-xmin_UTM)\n",
    "            self.nf_max    = xmax_UTM[indx]\n",
    "            self.nf_min    = xmin_UTM[indx]\n",
    "            self.sf        = (self.nf_max-self.nf_min)\n",
    "\n",
    "            self.crt_point = (xmax_UTM - xmin_UTM)/2 + xmin_UTM\n",
    "\n",
    "            if (type(Xp)!=type(None)) and (type(Yp)==type(None)):\n",
    "                for ii in [0,1,2]:\n",
    "                    Xp[:,ii]   = Xp[:,ii]   - self.crt_point[ii]\n",
    "                    Xp[:,ii+3] = Xp[:,ii+3] - self.crt_point[ii]\n",
    "                Xp = (Xp)/self.sf\n",
    "                return Xp\n",
    "            if (type(Xp)==type(None)) and (type(Yp)!=type(None)):\n",
    "                Yp  = Yp*self.sf\n",
    "                return Yp\n",
    "            else:\n",
    "                for ii in [0,1,2]:\n",
    "                    Xp[:,ii]   = Xp[:,ii]   - self.crt_point[ii]\n",
    "                    Xp[:,ii+3] = Xp[:,ii+3] - self.crt_point[ii]\n",
    "                Xp = (Xp)/self.sf\n",
    "                Yp = (Yp)/self.sf\n",
    "                return Xp,Yp\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # Initialising the network\n",
    "        self._init_network()\n",
    "\n",
    "        # Defining the optimization scheme\n",
    "        self.optimizer  = torch.optim.Adam(self.network.parameters(),lr=self.Params['Training']['Learning Rate'])\n",
    "        if self.Params['Training']['Use Scheduler (bool)'] == True:\n",
    "            self.scheduler  = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer)\n",
    "\n",
    "        # Creating a sampling dataset\n",
    "        self.dataset = Database(\n",
    "            self.Params['ModelPath'],\n",
    "            self.Params['VelocityClass'],\n",
    "            create=True,\n",
    "            Numsamples=int(self.Params['Training']['Number of sample points']),\n",
    "            randomDist=self.Params['Training']['Random Distance Sampling']\n",
    "        )\n",
    "        self.dataset.send_device(torch.device(self.Params['Device']))\n",
    "        self.dataset.data,self.dataset.target = self._normalization(Xp=self.dataset.data,Yp=self.dataset.target)\n",
    "\n",
    "        len_dataset         = len(self.dataset)\n",
    "        n_batches           = int(len(self.dataset)/int(self.Params['Training']['Batch Size']) + 1)\n",
    "        training_start_time = time.time()\n",
    "\n",
    "        # Splitting the dataset into training and validation\n",
    "        indices            = list(range(int(len_dataset)))\n",
    "        validation_idx     = np.random.choice(indices, size=int(len_dataset*(self.Params['Training']['Validation Percentage']/100)), replace=False)\n",
    "        train_idx          = list(set(indices) - set(validation_idx))\n",
    "        validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "        train_sampler      = SubsetRandomSampler(train_idx)\n",
    "\n",
    "        train_loader       = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=int(self.Params['Training']['Batch Size'] ),\n",
    "            sampler=train_sampler,\n",
    "            )    \n",
    "        validation_loader  = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=int(self.Params['Training']['Batch Size'] ),\n",
    "            sampler=validation_sampler,\n",
    "        )    \n",
    "\n",
    "        # Defining the initial weights to sample by\n",
    "        weights = Tensor(torch.ones(len(self.dataset))).to(torch.device(self.Params['Device']))\n",
    "        weights[validation_idx] = 0.0\n",
    "        print(weights.device)\n",
    "\n",
    "        for epoch in range(1,self.Params['Training']['Number of Epochs']+1):\n",
    "            print_every           = 1\n",
    "            start_time            = time.time()\n",
    "            running_sample_count  = 0\n",
    "            total_train_loss      = 0\n",
    "            total_val_loss        = 0\n",
    "\n",
    "            # Defining the weighting of the samples\n",
    "            weights                 = torch.clamp(weights/weights.max(),self.Params['Training']['Resampling Bounds'][0],self.Params['Training']['Resampling Bounds'][1])\n",
    "            weights[validation_idx] = 0.0\n",
    "            train_sampler_wei       = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "            train_loader_wei        = torch.utils.data.DataLoader(\n",
    "                                        self.dataset,\n",
    "                                        batch_size=int(self.Params['Training']['Batch Size'] ),\n",
    "                                        sampler=train_sampler_wei,\n",
    "                                      )\n",
    "            weights                 = Tensor(torch.zeros(len(self.dataset))).to(torch.device(self.Params['Device']))\n",
    "\n",
    "            for i, data in enumerate(train_loader_wei, 0):\n",
    "                \n",
    "                # Get inputs/outputs and wrap in variable object\n",
    "                inputs, labels, indexbatch = data\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.float()\n",
    "\n",
    "                inputs.requires_grad_()\n",
    "\n",
    "\n",
    "                if self.Params['Pytorch Amp (bool)']:\n",
    "                    with autocast():\n",
    "                        outputs = self.network(inputs)\n",
    "                        loss_value, wv  = EikonalLoss(labels,inputs,outputs,torch.device(self.Params['Device']))\n",
    "                else:\n",
    "                    outputs = self.network(inputs)\n",
    "                    loss_value, wv  = EikonalLoss(labels,inputs,outputs,torch.device(self.Params['Device']))\n",
    "\n",
    "\n",
    "                loss_value.backward()\n",
    "\n",
    "                # Update parameters\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Updating the weights\n",
    "                weights[indexbatch] = wv\n",
    "\n",
    "                total_train_loss += loss_value.item()\n",
    "                del inputs, labels, indexbatch, outputs, loss_value, wv\n",
    "\n",
    "\n",
    "            # Determining the Training Loss\n",
    "            for i, data_val in enumerate(validation_loader, 0):\n",
    "                inputs_val, labels_val, indexbatch_val = data_val\n",
    "                inputs_val = inputs_val.float()\n",
    "                labels_val = labels_val.float()\n",
    "                inputs_val.requires_grad_()\n",
    "\n",
    "                if self.Params['Pytorch Amp (bool)']:\n",
    "                    with autocast():\n",
    "                        outputs_val                 = self.network(inputs_val)\n",
    "                        val_loss,wv                 = EikonalLoss(labels_val,inputs_val,outputs_val,torch.device(self.Params['Device']))\n",
    "                else:\n",
    "                    outputs_val  = self.network(inputs_val)\n",
    "                    val_loss,wv  = EikonalLoss(labels_val,inputs_val,outputs_val,torch.device(self.Params['Device']))\n",
    "\n",
    "                total_val_loss             += val_loss.item()\n",
    "                del inputs_val, labels_val, indexbatch_val, outputs_val, val_loss, wv\n",
    "\n",
    "\n",
    "            # Creating a running loss for both training and validation data\n",
    "            total_val_loss   /= len(validation_loader)\n",
    "            total_train_loss /= len(train_loader)\n",
    "            self.total_train_loss.append(total_train_loss)\n",
    "            self.total_val_loss.append(total_val_loss)\n",
    "\n",
    "            if self.Params['Training']['Use Scheduler (bool)'] == True:\n",
    "                self.scheduler.step(total_val_loss)\n",
    "\n",
    "            del train_loader_wei,train_sampler_wei\n",
    "\n",
    "            if epoch % self.Params['Training']['Print Every * Epoch'] == 0:\n",
    "                with torch.no_grad():\n",
    "                    print(\"Epoch = {} -- Training loss = {:.4e} -- Validation loss = {:.4e}\".format(epoch,total_train_loss,total_val_loss))\n",
    "\n",
    "            if (epoch % self.Params['Training']['Save Every * Epoch'] == 0) or (epoch == self.Params['Training']['Number of Epochs'] ) or (epoch == 1):\n",
    "                with torch.no_grad():\n",
    "                    self.save(epoch=epoch,val_loss=total_val_loss)\n",
    "\n",
    "    def save(self,epoch='',val_loss=''):\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch':epoch,\n",
    "                'model_state_dict': self.network.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'train_loss': self.total_train_loss,\n",
    "                'val_loss': self.total_val_loss\n",
    "            }, \n",
    "            '{}/Model_Epoch_{}_ValLoss_{}.pt'.format(self.Params['ModelPath'],str(epoch).zfill(5),val_loss)\n",
    "        )\n",
    "\n",
    "    def load(self,filepath):\n",
    "        # -- Loading model information\n",
    "        self._init_network()\n",
    "        checkpoint            = torch.load(filepath,map_location=torch.device(self.Params['Device']))\n",
    "        self.total_train_loss = checkpoint['train_loss']\n",
    "        self.total_val_loss   = checkpoint['val_loss']\n",
    "        self.network.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.network.to(torch.device(self.Params['Device']))\n",
    "\n",
    "\n",
    "    def TravelTimes(self,Xp,projection=True,normlisation=True):\n",
    "        # Apply projection from LatLong to UTM\n",
    "        Xp  = Xp.to(torch.device(self.Params['Device']))\n",
    "        if projection:\n",
    "            Xp  = self._projection(Xp)\n",
    "        if normlisation:\n",
    "            Xp  = self._normalization(Xp=Xp,Yp=None)\n",
    "        tau = self.network(Xp)\n",
    "        T0  = torch.sqrt(((Xp[:,3]-Xp[:,0])**2 + (Xp[:,4]-Xp[:,1])**2 + (Xp[:,5]-Xp[:,2])**2))\n",
    "        TT  = tau[:,0]*T0\n",
    "        del Xp,tau,T0\n",
    "        return TT\n",
    "\n",
    "    def Velocity(self,Xp,projection=True,normlisation=True):\n",
    "        Xp    = Xp.to(torch.device(self.Params['Device']))\n",
    "        if projection:\n",
    "            Xp  = self._projection(Xp)\n",
    "        if normlisation:\n",
    "            Xp  = self._normalization(Xp=Xp,Yp=None)\n",
    "        Xp.requires_grad_()\n",
    "        tau   = self.network(Xp)\n",
    "        dtau  = torch.autograd.grad(outputs=tau, inputs=Xp, grad_outputs=torch.ones(tau.size()).to(torch.device(self.Params['Device'])), \n",
    "                                    only_inputs=True,create_graph=True,retain_graph=True)[0]        \n",
    "        T0    = torch.sqrt(((Xp[:,3]-Xp[:,0])**2 + (Xp[:,4]-Xp[:,1])**2 + (Xp[:,5]-Xp[:,2])**2))  \n",
    "        T1    = (T0**2)*(dtau[:,3]**2 + dtau[:,4]**2 + dtau[:,5]**2)\n",
    "        T2    = 2*tau[:,0]*(dtau[:,3]*(Xp[:,3]-Xp[:,0]) + dtau[:,4]*(Xp[:,4]-Xp[:,1]) + dtau[:,5]*(Xp[:,5]-Xp[:,2]))\n",
    "        T3    = tau[:,0]**2\n",
    "        Ypred = torch.sqrt(1/(T1+T2+T3))\n",
    "        Ypred = self._normalization(Yp=Ypred)\n",
    "        del Xp,tau,dtau,T0,T1,T2,T3\n",
    "        return Ypred\n",
    "\n",
    "class _numpy2dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        # Creating identical pairs\n",
    "        self.data    = Variable(Tensor(data))\n",
    "        self.target  = Variable(Tensor(target))\n",
    "\n",
    "    def send_device(self,device):\n",
    "        self.data    = self.data.to(device)\n",
    "        self.target  = self.target.to(device)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        return x, y, index\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "def _randPoints(numsamples=10000,randomDist=False,Xmin=[0,0,0],Xmax=[2,2,2]):\n",
    "    numsamples = int(numsamples)\n",
    "    Xmin = np.append(Xmin,Xmin)\n",
    "    Xmax = np.append(Xmax,Xmax)\n",
    "    if randomDist:\n",
    "        X  = np.zeros((numsamples,6))\n",
    "        PointsOutside = np.arange(numsamples)\n",
    "        while len(PointsOutside) > 0:\n",
    "            P  = np.random.rand(len(PointsOutside),3)*(Xmax[:3]-Xmin[:3])[None,None,:] + Xmin[:3][None,None,:]\n",
    "            dP = np.random.rand(len(PointsOutside),3)-0.5\n",
    "            rL = (np.random.rand(len(PointsOutside),1))*np.sqrt(np.sum((Xmax-Xmin)**2))\n",
    "            nP = P + (dP/np.sqrt(np.sum(dP**2,axis=1))[:,np.newaxis])*rL\n",
    "\n",
    "            X[PointsOutside,:3] = P\n",
    "            X[PointsOutside,3:] = nP\n",
    "\n",
    "            maxs          = np.any((X[:,3:] > Xmax[:3][None,:]),axis=1)\n",
    "            mins          = np.any((X[:,3:] < Xmin[:3][None,:]),axis=1)\n",
    "            OutOfDomain   = np.any(np.concatenate((maxs[:,None],mins[:,None]),axis=1),axis=1)\n",
    "            PointsOutside = np.where(OutOfDomain)[0]\n",
    "    else:\n",
    "        X  = (np.random.rand(numsamples,6)*(Xmax-Xmin)[None,None,:] + Xmin[None,None,:])[0,:,:]\n",
    "    return X\n",
    "\n",
    "\n",
    "def Database(PATH,VelocityFunction,create=False,Numsamples=5000,randomDist=False,SurfaceRecievers=False):\n",
    "    if create == True:\n",
    "        xmin = copy.copy(VelocityFunction.xmin)\n",
    "        xmax = copy.copy(VelocityFunction.xmax)\n",
    "\n",
    "        # Projecting from LatLong to UTM\n",
    "        if type(VelocityFunction.projection) == str:\n",
    "            proj = Proj(VelocityFunction.projection)\n",
    "            xmin[0],xmin[1] = proj(xmin[0],xmin[1])\n",
    "            xmax[0],xmax[1] = proj(xmax[0],xmax[1])\n",
    "\n",
    "        Xp   = _randPoints(numsamples=Numsamples,Xmin=xmin,Xmax=xmax,randomDist=randomDist)\n",
    "        Yp   = VelocityFunction.eval(Xp)\n",
    "\n",
    "        # Handling NaNs values\n",
    "        while len(np.where(np.isnan(Yp[:,1]))[0]) > 0:\n",
    "            indx     = np.where(np.isnan(Yp[:,1]))[0]\n",
    "            print('Recomputing for {} points with nans'.format(len(indx)))\n",
    "            Xpi      = _randPoints(numsamples=len(indx),Xmin=xmin,Xmax=xmax,randomDist=randomDist)\n",
    "            Yp[indx,:] = VelocityFunction.eval(Xpi)\n",
    "            Xp[indx,:] = Xpi\n",
    "\n",
    "        # Saving the training dataset\n",
    "        np.save('{}/Xp'.format(PATH),Xp)\n",
    "        np.save('{}/Yp'.format(PATH),Yp)\n",
    "    else:\n",
    "        try:\n",
    "            Xp = np.load('{}/Xp.npy'.format(PATH))\n",
    "            Yp = np.load('{}/Yp.npy'.format(PATH))\n",
    "        except ValueError:\n",
    "            print('Please specify a correct source path, or create a dataset')\n",
    "\n",
    "\n",
    "    print(Xp.shape,Yp.shape)\n",
    "    database = _numpy2dataset(Xp,Yp)\n",
    "\n",
    "    return database\n",
    "\n",
    "class ToyProblem_Checkerboard:\n",
    "    def __init__(self):\n",
    "        self.xmin     = [0,0,0]\n",
    "        self.xmax     = [20.,20.,20.]\n",
    "\n",
    "        # projection \n",
    "        self.projection = None\n",
    "\n",
    "        # Velocity values\n",
    "        self.velocity_mean     = 5.0\n",
    "        self.velocity_phase    = 0.5\n",
    "        self.velocity_offset   = -2.5 \n",
    "        self.velcoity_amp      = 1.0\n",
    "\n",
    "    def eval(self,Xp):\n",
    "        Yp = np.ones((Xp.shape[0],2))\n",
    "        SinS = (signal.square(Xp[:,0]+self.velocity_offset,self.velocity_phase) + signal.square(Xp[:,1]+self.velocity_offset,self.velocity_phase) + signal.square(Xp[:,2]+self.velocity_offset,self.velocity_phase))/3\n",
    "        SinR = (signal.square(Xp[:,3]+self.velocity_offset,self.velocity_phase) + signal.square(Xp[:,4]+self.velocity_offset,self.velocity_phase) + signal.square(Xp[:,5]+self.velocity_offset,self.velocity_phase))/3\n",
    "        Yp[:,0] = (SinS)*self.velcoity_amp + self.velocity_mean\n",
    "        Yp[:,1] = (SinR)*self.velcoity_amp + self.velocity_mean\n",
    "        return Yp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "filePath = './'\n",
    "model = Model(filePath,VelocityClass=ToyProblem_Checkerboard(),device='cuda:0')\n",
    "model.train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/taufikmh/miniconda3/envs/thesis-pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10000, 6) (10000, 2)\n",
      "cuda:0\n",
      "Epoch = 1 -- Training loss = 1.5567e-01 -- Validation loss = 9.5890e-02\n",
      "Epoch = 2 -- Training loss = 1.0995e-01 -- Validation loss = 9.8864e-02\n",
      "Epoch = 3 -- Training loss = 1.4561e-01 -- Validation loss = 1.1124e-01\n",
      "Epoch = 4 -- Training loss = 1.4514e-01 -- Validation loss = 9.7096e-02\n",
      "Epoch = 5 -- Training loss = 1.5494e-01 -- Validation loss = 1.1705e-01\n",
      "Epoch = 6 -- Training loss = 1.4851e-01 -- Validation loss = 1.3032e-01\n",
      "Epoch = 7 -- Training loss = 1.4755e-01 -- Validation loss = 9.5921e-02\n",
      "Epoch = 8 -- Training loss = 1.4881e-01 -- Validation loss = 9.8792e-02\n",
      "Epoch = 9 -- Training loss = 1.5046e-01 -- Validation loss = 1.4586e-01\n",
      "Epoch = 10 -- Training loss = 1.4075e-01 -- Validation loss = 1.3226e-01\n",
      "Epoch = 11 -- Training loss = 1.4839e-01 -- Validation loss = 9.9825e-02\n",
      "Epoch = 12 -- Training loss = 1.4717e-01 -- Validation loss = 9.9187e-02\n",
      "Epoch = 13 -- Training loss = 1.3946e-01 -- Validation loss = 9.8860e-02\n",
      "Epoch = 14 -- Training loss = 1.3581e-01 -- Validation loss = 1.0235e-01\n",
      "Epoch = 15 -- Training loss = 1.4489e-01 -- Validation loss = 1.0373e-01\n",
      "Epoch = 16 -- Training loss = 1.3572e-01 -- Validation loss = 1.0173e-01\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3458/51352256.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilePath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVelocityClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mToyProblem_Checkerboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3458/3655818667.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis-pytorch/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis-pytorch/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis-pytorch/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis-pytorch/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# velocity_function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def velocity_function(vel_sha='sphere', vel_typ='homogeneous', vel_ini=5, x=X, y=Y, z=Z):\n",
    "    if vel_sha=='sphere':\n",
    "        # inhomogeneous velocity without gaps\n",
    "        vel_inh = vel_ini - (x**2 + z**2 + y**2)\n",
    "        vel_inh[vel_inh<2.**2] = np.NaN\n",
    "        if vel_typ=='inhomogeneous':  \n",
    "            # velocity without gaps\n",
    "            vel_all = np.copy(vel_inh)\n",
    "            \n",
    "            # velocity with gaps\n",
    "            vel_gap = np.copy(vel_all)\n",
    "            vel_gap[vel_inh>2.1**2] = np.NaN\n",
    "        elif vel_typ == 'random':\n",
    "            # velocity without gaps\n",
    "            from scipy.ndimage import gaussian_filter\n",
    "            vel_all = gaussian_filter(18*np.random.random((nx,ny,nz))**3, sigma=6)\n",
    "            vel_all[np.isnan(vel_inh)] = np.NaN\n",
    "\n",
    "            # velocity with gaps\n",
    "            vel_gap = np.copy(vel_all)\n",
    "            vel_gap[vel_inh>2.1**2] = np.NaN\n",
    "        elif vel_typ == 'homogeneous':\n",
    "            # velocity without gaps\n",
    "            vel_all = vel_ini*np.ones_like(vel_inh)\n",
    "            vel_all[np.isnan(vel_inh)] = np.NaN\n",
    "            \n",
    "            # velocity with gaps\n",
    "            vel_tmp = np.copy(vel_inh)\n",
    "            vel_tmp[vel_inh>2.1**2] = np.NaN\n",
    "            vel_gap = np.copy(vel_all)\n",
    "            vel_gap[np.isnan(vel_tmp)] = np.NaN\n",
    "        elif vel_typ == 'radial':\n",
    "            from generate_data import vp_gap\n",
    "            vel_inh = vp\n",
    "            vel_all = vp\n",
    "            vel_gap = vp_gap\n",
    "            \n",
    "            \n",
    "    return vel_inh, vel_all, vel_gap\n",
    "\n",
    "# call function\n",
    "vel_inh, vel_all, vel_gap = velocity_function(vel_typ=vel_typ)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# training_points"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## V(x,y,z)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# define receiver coordinates\n",
    "xR, yR, zR = X.reshape(-1,1), Y.reshape(-1,1), Z.reshape(-1,1)\n",
    "\n",
    "# define source coordinates\n",
    "xS, yS, zS = sx*np.ones_like(X.reshape(-1,1)), sy*np.ones_like(X.reshape(-1,1)), sz*np.ones_like(X.reshape(-1,1))\n",
    "\n",
    "# define inputs and output\n",
    "Xp = np.hstack((xS, yS, zS, xR, yR, zR))\n",
    "yp = vel_mod.reshape(-1,1)\n",
    "\n",
    "# random sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    Xp[np.logical_not(np.isnan(yp))[:,0]], \n",
    "    yp[np.logical_not(np.isnan(yp))].reshape(-1,1), \n",
    "    test_size=0.33,\n",
    "    random_state=125\n",
    ")\n",
    "\n",
    "# TensorFlow data pipeline\n",
    "dat_tra = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dat_tra = dat_tra.batch(X_train.shape[0]+1)\n",
    "dat_val = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "dat_val = dat_val.batch(X_test.shape[0]+1)                       \n",
    "    \n",
    "X_starf = [X_train[:,3].reshape(-1,1), X_train[:,4].reshape(-1,1), X_train[:,5].reshape(-1,1)]\n",
    "\n",
    "# find source location id in X_star\n",
    "TOL = 1e-11\n",
    "sids,_ = np.where((np.isclose(X_starf[0], sx)) & (np.isclose(X_starf[1], sy)) & (np.isclose(X_starf[2], sz)))\n",
    "\n",
    "print(sids)\n",
    "print(sids.shape)\n",
    "print(X_starf[0][sids,0])\n",
    "print(X_starf[1][sids,0])\n",
    "print(X_starf[2][sids,0])\n",
    "print(sx,sy,sz)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[43834]\n",
      "(1,)\n",
      "[1.]\n",
      "[0.]\n",
      "[1.]\n",
      "1.0 0.0 1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## V(rad,the,phi)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define receiver coordinates\n",
    "xR, yR, zR = X.reshape(-1,1), Y.reshape(-1,1), Z.reshape(-1,1)\n",
    "\n",
    "# define source coordinates\n",
    "xS, yS, zS = sx*np.ones_like(X.reshape(-1,1)), sy*np.ones_like(X.reshape(-1,1)), sz*np.ones_like(X.reshape(-1,1))\n",
    "\n",
    "# define inputs and output\n",
    "Xp = np.hstack((xS, yS, zS, xR, yR, zR))\n",
    "yp = vel_gap.reshape(-1,1)\n",
    "\n",
    "# random sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    Xp[np.logical_not(np.isnan(yp))[:,0]], \n",
    "    yp[np.logical_not(np.isnan(yp))].reshape(-1,1),\n",
    "    test_size=0.3,\n",
    "    random_state=1235\n",
    ")\n",
    "\n",
    "# find source location id in X_star\n",
    "sids,_ = np.where(\n",
    "    (np.isclose(X_train[:,3].reshape(-1,1), sx)) & \n",
    "    (np.isclose(X_train[:,4].reshape(-1,1), sy)) & \n",
    "    (np.isclose(X_train[:,5].reshape(-1,1), sz))\n",
    ")\n",
    "\n",
    "print(sids)\n",
    "print(sx,sy,sz)\n",
    "\n",
    "# TensorFlow data pipeline\n",
    "dat_tra = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dat_tra = dat_tra.batch(bat_siz)\n",
    "dat_val = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "dat_val = dat_val.batch(bat_siz) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "with strategy.scope():\n",
    "    \n",
    "    # custom model compilation\n",
    "    inp_eik = tf.keras.layers.Input(shape=(6,))\n",
    "    for_net = feedforward_model.build()\n",
    "    gra_lay = backpropagation_layer(for_net)\n",
    "    out_eik = custom_ouput(gra_lay, inp_eik)\n",
    "\n",
    "    # compile model\n",
    "    cus_mod = custom_model([inp_eik], [out_eik])\n",
    "    cus_mod.compile(optimizer=\"adam\", metrics=[\"mse\"])\n",
    "    \n",
    "    # callbacks\n",
    "    pocket = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        min_delta=0.001,\n",
    "        patience=5, \n",
    "        verbose=1,\n",
    "        restore_best_weights = True\n",
    "    )\n",
    "\n",
    "    history = cus_mod.fit(\n",
    "        dat_tra,\n",
    "        epochs=3,\n",
    "        callbacks=[pocket],\n",
    "        validation_data=dat_val,\n",
    "        workers=3, \n",
    "        use_multiprocessing=True\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# custom_training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# build a core tau model\n",
    "tau = feedforward_model.build()\n",
    "\n",
    "# build a PINN model\n",
    "model = PINNs_model(tau).build()\n",
    "model.summary()\n",
    "\n",
    "lea_sch = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=400,\n",
    "    decay_rate=0.90\n",
    ")\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lea_sch)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lea_rat)\n",
    "train_loss_results = []\n",
    "test_loss_results = []\n",
    "num_epochs = num_epo\n",
    "train_acc_metric = tf.keras.metrics.MeanSquaredError(name=\"mse\")\n",
    "test_acc_metric = tf.keras.metrics.MeanSquaredError(name=\"mse\")\n",
    "\n",
    "# learning_function = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=lea_rat,\n",
    "#     decay_steps=num_epo\n",
    "# )\n",
    "\n",
    "@tf.function\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def train_step(model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = tf.reduce_mean(tf.keras.losses.mse(model(x, training=True), y))\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    train_acc_metric.update_state(model(x, training=True), y)\n",
    "    return loss_value\n",
    "\n",
    "@tf.function\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def test_step(model, x, y):\n",
    "    loss_value = tf.reduce_mean(tf.keras.losses.mse(model(x, training=False), y))\n",
    "    val_acc_metric.update_state(model(x, training=False), y)\n",
    "    return loss_value\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs+1):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    \n",
    "    # loop over batches\n",
    "    for x, y in dat_tra:\n",
    "        train_loss_value = train_step(model, x, y)\n",
    "        # track progress\n",
    "        epoch_loss_avg.update_state(train_loss_value)\n",
    "    \n",
    "#     train_loss_value = train_step(model, X_train, y_train)\n",
    "#     # track progress\n",
    "#     epoch_loss_avg.update_state(train_loss_value)\n",
    "    \n",
    "    # end epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Epoch {:03d}: Loss: {:.6f}\".format(epoch, epoch_loss_avg.result()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(None, 1)\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_25 (Sl (None, 1)            0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_22 (Sl (None, 1)            0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_26 (Sl (None, 1)            0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_23 (Sl (None, 1)            0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_18 (TFOpLambda (None, 1)            0           tf.__operators__.getitem_25[0][0]\n",
      "                                                                 tf.__operators__.getitem_22[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_19 (TFOpLambda (None, 1)            0           tf.__operators__.getitem_26[0][0]\n",
      "                                                                 tf.__operators__.getitem_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_27 (Sl (None, 1)            0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_24 (Sl (None, 1)            0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.pow_24 (TFOpLambda)     (None, 1)            0           tf.math.subtract_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.pow_25 (TFOpLambda)     (None, 1)            0           tf.math.subtract_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_20 (TFOpLambda (None, 1)            0           tf.__operators__.getitem_27[0][0]\n",
      "                                                                 tf.__operators__.getitem_24[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_24 (TFOpLa (None, 1)            0           tf.math.pow_24[0][0]             \n",
      "                                                                 tf.math.pow_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.pow_26 (TFOpLambda)     (None, 1)            0           tf.math.subtract_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "backpropagation_layer_3 (backpr ((None, 1), (None, 1 7425537     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_21 (TFOpLambda (None, 1)            0           tf.__operators__.getitem_25[0][0]\n",
      "                                                                 tf.__operators__.getitem_22[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_22 (TFOpLambda (None, 1)            0           tf.__operators__.getitem_26[0][0]\n",
      "                                                                 tf.__operators__.getitem_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_25 (TFOpLa (None, 1)            0           tf.__operators__.add_24[0][0]    \n",
      "                                                                 tf.math.pow_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.pow_28 (TFOpLambda)     (None, 1)            0           backpropagation_layer_3[0][1]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.pow_29 (TFOpLambda)     (None, 1)            0           backpropagation_layer_3[0][2]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_20 (TFOpLambda (None, 1)            0           backpropagation_layer_3[0][1]    \n",
      "                                                                 tf.math.subtract_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_21 (TFOpLambda (None, 1)            0           backpropagation_layer_3[0][2]    \n",
      "                                                                 tf.math.subtract_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_23 (TFOpLambda (None, 1)            0           tf.__operators__.getitem_27[0][0]\n",
      "                                                                 tf.__operators__.getitem_24[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sqrt_9 (TFOpLambda)     (None, 1)            0           tf.__operators__.add_25[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_26 (TFOpLa (None, 1)            0           tf.math.pow_28[0][0]             \n",
      "                                                                 tf.math.pow_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.pow_30 (TFOpLambda)     (None, 1)            0           backpropagation_layer_3[0][3]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_28 (TFOpLa (None, 1)            0           tf.math.multiply_20[0][0]        \n",
      "                                                                 tf.math.multiply_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_22 (TFOpLambda (None, 1)            0           backpropagation_layer_3[0][3]    \n",
      "                                                                 tf.math.subtract_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.pow_27 (TFOpLambda)     (None, 1)            0           tf.math.sqrt_9[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_27 (TFOpLa (None, 1)            0           tf.__operators__.add_26[0][0]    \n",
      "                                                                 tf.math.pow_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_19 (TFOpLambda (None, 1)            0           backpropagation_layer_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_29 (TFOpLa (None, 1)            0           tf.__operators__.add_28[0][0]    \n",
      "                                                                 tf.math.multiply_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_18 (TFOpLambda (None, 1)            0           tf.math.pow_27[0][0]             \n",
      "                                                                 tf.__operators__.add_27[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_23 (TFOpLambda (None, 1)            0           tf.math.multiply_19[0][0]        \n",
      "                                                                 tf.__operators__.add_29[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_30 (TFOpLa (None, 1)            0           tf.math.multiply_18[0][0]        \n",
      "                                                                 tf.math.multiply_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.pow_31 (TFOpLambda)     (None, 1)            0           backpropagation_layer_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_31 (TFOpLa (None, 1)            0           tf.__operators__.add_30[0][0]    \n",
      "                                                                 tf.math.pow_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_7 (TFOpLambda)  (None, 1)            0           tf.__operators__.add_31[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sqrt_11 (TFOpLambda)    (None, 1)            0           tf.math.truediv_7[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 7,425,537\n",
      "Trainable params: 7,393,793\n",
      "Non-trainable params: 31,744\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 000: Loss: 6.254594\n",
      "Epoch 050: Loss: 4.532739\n",
      "Epoch 100: Loss: 5.673643\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44517/1757817601.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# loop over batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdat_tra\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mtrain_loss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;31m# track progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mepoch_loss_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis-sciann/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis-sciann/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis-sciann/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis-sciann/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/envs/thesis-sciann/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis-sciann/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 2d plot\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "im = ax.imshow(\n",
    "    np.abs(T_pred[X.shape[0]//2,:,:]-T_data[X.shape[0]//2,:,:]), \n",
    "    extent=[np.min(x_plot),np.max(x_plot),np.max(z_plot),np.min(z_plot)], \n",
    "    aspect=1,     \n",
    "    cmap=\"jet\"\n",
    ")\n",
    "plt.xlabel('Z (x10^3 km)')\n",
    "plt.ylabel('X (x10^3 km)')\n",
    "plt.title('Absolute Travel Time Reisudals')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "ax.plot(sz*ear_rad/1000,sx*ear_rad/1000,'w*',markersize=8)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"6%\", pad=0.15)\n",
    "\n",
    "cbar = plt.colorbar(im, cax=cax, format='%.3e')\n",
    "cbar.set_label('s',size=10)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "# cbar.mappable.set_clim(np.min(T_fmm_all), np.max(T_fmm_all[np.isfinite(T_fmm_all)]))\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 2d plot\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "im = ax.imshow(\n",
    "    T_data[X.shape[0]//2,:,:], \n",
    "    extent=[np.min(x_plot),np.max(x_plot),np.max(z_plot),np.min(z_plot)], \n",
    "    aspect=1,     \n",
    "    cmap=\"jet\"\n",
    ")\n",
    "plt.xlabel('Z (x10^3 km)')\n",
    "plt.ylabel('X (x10^3 km)')\n",
    "plt.title('Analytical Travel Time')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "ax.plot(sz*ear_rad/1000,sx*ear_rad/1000,'w*',markersize=8)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"6%\", pad=0.15)\n",
    "\n",
    "cbar = plt.colorbar(im, cax=cax, format='%.3e')\n",
    "cbar.set_label('s',size=10)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "# cbar.mappable.set_clim(np.min(T_fmm_all), np.max(T_fmm_all[np.isfinite(T_fmm_all)]))\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 2d plot\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "im = ax.imshow(\n",
    "    T_pred[X.shape[0]//2,:,:], \n",
    "    extent=[np.min(x_plot),np.max(x_plot),np.max(z_plot),np.min(z_plot)], \n",
    "    aspect=1,     \n",
    "    cmap=\"jet\"\n",
    ")\n",
    "plt.xlabel('Z (x10^3 km)')\n",
    "plt.ylabel('X (x10^3 km)')\n",
    "plt.title('Predicted Travel Time')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "ax.plot(sz*ear_rad/1000,sx*ear_rad/1000,'w*',markersize=8)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"6%\", pad=0.15)\n",
    "\n",
    "cbar = plt.colorbar(im, cax=cax, format='%.3e')\n",
    "cbar.set_label('s',size=10)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "# cbar.mappable.set_clim(np.min(T_fmm_all), np.max(T_fmm_all[np.isfinite(T_fmm_all)]))\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 2d plot\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "im = ax.imshow(\n",
    "    np.abs(V_pred[X.shape[0]//2,:,:]-vel_mod[X.shape[0]//2,:,:]), \n",
    "    extent=[np.min(x_plot),np.max(x_plot),np.max(z_plot),np.min(z_plot)], \n",
    "    aspect=1,     \n",
    "    cmap=\"jet\"\n",
    ")\n",
    "plt.xlabel('Z (x10^3 km)')\n",
    "plt.ylabel('X (x10^3 km)')\n",
    "plt.title('Absolute Velocity Resiudals')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "ax.plot(sz*ear_rad/1000,sx*ear_rad/1000,'w*',markersize=8)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"6%\", pad=0.15)\n",
    "\n",
    "cbar = plt.colorbar(im, cax=cax, format='%.3e')\n",
    "cbar.set_label('km/s',size=10)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "# cbar.mappable.set_clim(2, 3)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 2d\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "im = ax.pcolormesh(\n",
    "    xx[:,latitude.shape[0]//2,:],\n",
    "    yy[:,latitude.shape[0]//2,:],\n",
    "    V_pred_gap[::-1,latitude.shape[0]//2,:],\n",
    "    cmap=plt.get_cmap(\"jet\"),\n",
    "    shading=\"gouraud\"\n",
    ")\n",
    "ax.set_aspect('equal')\n",
    "plt.xlabel('Z (x10^3 km)')\n",
    "plt.ylabel('X (x10^3 km)')\n",
    "plt.title('Predicted Velocity (Gap)')\n",
    "ax.plot(xx_s,yy_s,'w*',markersize=8)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"6%\", pad=0.15)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "\n",
    "cbar = plt.colorbar(im, cax=cax, format='%.3e')\n",
    "cbar.set_label('km/s',size=10)\n",
    "cbar.mappable.set_clim(v_min, v_max)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "plt.savefig(figures_path + 'V_PINNs_Gap_2d.png', bbox_inches=\"tight\")\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 2d\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "im = ax.pcolormesh(\n",
    "    xx[:,latitude.shape[0]//2,:],\n",
    "    yy[:,latitude.shape[0]//2,:],\n",
    "    V_pred[::-1,latitude.shape[0]//2,:],\n",
    "    cmap=plt.get_cmap(\"jet\"),\n",
    "    shading=\"gouraud\"\n",
    ")\n",
    "ax.set_aspect('equal')\n",
    "plt.xlabel('Z (x10^3 km)')\n",
    "plt.ylabel('X (x10^3 km)')\n",
    "plt.title('Predicted Velocity (All)')\n",
    "ax.plot(xx_s,yy_s,'w*',markersize=8)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"6%\", pad=0.15)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "\n",
    "cbar = plt.colorbar(im, cax=cax, format='%.3e')\n",
    "cbar.set_label('km/s',size=10)\n",
    "cbar.mappable.set_clim(v_min, v_max)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "plt.savefig(figures_path + 'V_PINNs_all_2d.png', bbox_inches=\"tight\")\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.pcolormesh(\n",
    "    xx[:,latitude.shape[0]//2,:],\n",
    "    yy[:,latitude.shape[0]//2,:],\n",
    "    vel_all[::-1,latitude.shape[0]//2,:],\n",
    "    cmap=plt.get_cmap(\"jet\"),\n",
    "    shading=\"gouraud\"\n",
    ")\n",
    "ax.set_aspect('equal')\n",
    "plt.xlabel('Z (x10^3 km)')\n",
    "plt.ylabel('X (x10^3 km)')\n",
    "plt.title('Initial Velocity (All)')\n",
    "ax.plot(xx_s,yy_s,'w*',markersize=8)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"6%\", pad=0.15)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "\n",
    "cbar = plt.colorbar(im, cax=cax, format='%.3e')\n",
    "cbar.set_label('km/s',size=10)\n",
    "cbar.mappable.set_clim(v_min, v_max)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "plt.savefig(figures_path + 'V_ini_all_2d.png', bbox_inches=\"tight\")\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 2d\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "im = ax.pcolormesh(\n",
    "    xx[:,latitude.shape[0]//2,:],\n",
    "    yy[:,latitude.shape[0]//2,:],\n",
    "    vel_gap[::-1,latitude.shape[0]//2,:],\n",
    "    cmap=plt.get_cmap(\"jet\"),\n",
    "    shading=\"gouraud\"\n",
    ")\n",
    "ax.set_aspect('equal')\n",
    "plt.xlabel('Z (x10^3 km)')\n",
    "plt.ylabel('X (x10^3 km)')\n",
    "plt.title('Initial Velocity (Gap)')\n",
    "ax.plot(xx_s,yy_s,'w*',markersize=8)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"6%\", pad=0.15)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "\n",
    "cbar = plt.colorbar(im, cax=cax, format='%.3e')\n",
    "cbar.set_label('km/s',size=10)\n",
    "cbar.mappable.set_clim(v_min, v_max)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "plt.savefig(figures_path + 'V_ini_gap_2d.png', bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_, DEP, _ = np.meshgrid(latitude, depth, longitude)\n",
    "\n",
    "xx = DEP * np.sin(np.radians(LAT+90)) * np.cos(np.radians(180+LON))/(1e3)\n",
    "yy = DEP * np.sin(np.radians(LAT+90)) * np.sin(np.radians(180+LON))/(1e3)\n",
    "\n",
    "xx_s = (ear_rad - dep_sou) * np.sin(np.radians(lat_sou+90)) * np.cos(np.radians(180+lon_sou))/(1e3)\n",
    "yy_s = (ear_rad - dep_sou) * np.sin(np.radians(lat_sou+90)) * np.sin(np.radians(180+lon_sou))/(1e3)\n",
    "\n",
    "v_min, v_max = np.min(vel_all), np.max(vel_all)\n",
    "\n",
    "\n",
    "V_pred_gap = np.where(DEP<3000, V_pred, np.nan)\n",
    "T_pred_gap = np.where(DEP<3000, T_pred, np.nan)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# plots"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tau_pred = tau.predict(Xp, batch_size=64).reshape((dep_dim, lat_dim, lon_dim))\n",
    "V_pred = model.predict(Xp, batch_size=64).reshape((dep_dim, lat_dim, lon_dim))\n",
    "T0 = tf.math.sqrt((X-sx)**2 + (Y-sy)**2 + (Z-sz)**2)\n",
    "T_pred = tau_pred * T0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tau_pred = tau.predict(Xp, batch_size=64).reshape((nx, ny, nz))\n",
    "V_pred = model.predict(Xp, batch_size=64).reshape((nx, ny, nz))\n",
    "T0 = tf.math.sqrt((X-sx)**2 + (Y-sy)**2 + (Z-sz)**2)\n",
    "T_pred = tau_pred * T0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 2d\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "im = plt.imshow(np.abs(V_pred[:,20,:]-vel_mod[:,20,:]))\n",
    "ax.set_aspect('equal')\n",
    "plt.xlabel('Z (x10^3 km)')\n",
    "plt.ylabel('X (x10^3 km)')\n",
    "plt.title('Absolute Velocity Residuals')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"6%\", pad=0.15)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "\n",
    "cbar = plt.colorbar(im, cax=cax, format='%.3e')\n",
    "cbar.set_label('km/s',size=10)\n",
    "# cbar.mappable.set_clim(2, 3)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tf.keras.optimizers.schedules.PiecewiseConstantDecay?"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('thesis-pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "25e09450674d4009128f9a78adc3c2771164b217a484aaede1fffef6f3aee8e3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}