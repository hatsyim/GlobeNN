{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import datetime\n",
    "import random\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import netCDF4\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.colors as mcolors\n",
    "import h5py\n",
    "import argparse\n",
    "import pymap3d as pm\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import copy\n",
    "import re\n",
    "import matplotlib.patches as patches\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "from scipy import signal\n",
    "from scipy import spatial\n",
    "from torch.nn import Linear\n",
    "from torch import Tensor\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data.sampler import SubsetRandomSampler,WeightedRandomSampler\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from pyproj import Proj\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from glob import glob\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes,mark_inset\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "# for reproducibility\n",
    "os.environ['PYTHONHASHSEED']= '123'\n",
    "np.random.seed(123)\n",
    "\n",
    "# use custom Matplotlib style\n",
    "plt.style.use(\"./science.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = '../../00_data/glad-m25-vp-0.0-n4.nc'\n",
    "\n",
    "data = xr.open_dataset(path_file)\n",
    "\n",
    "offline = False\n",
    "old = False\n",
    "\n",
    "if offline and old:\n",
    "    dep_ini, dep_inc = 11, 4\n",
    "    lat_ini, lat_inc = 0, 8\n",
    "    lon_ini, lon_inc = 0, 8\n",
    "else:\n",
    "    dep_ini, dep_inc = 11, 2\n",
    "    lat_ini, lat_inc = 0, 3\n",
    "    lon_ini, lon_inc = 0, 3    \n",
    "\n",
    "# variables\n",
    "vpv = data.variables['vpv'].values[dep_ini::dep_inc, lat_ini::lat_inc, lon_ini::lon_inc]\n",
    "vph = data.variables['vph'].values[dep_ini::dep_inc, lat_ini::lat_inc, lon_ini::lon_inc]\n",
    "longitude = data.variables['longitude'].values[lon_ini::lon_inc]\n",
    "latitude = data.variables['latitude'].values[lat_ini::lat_inc]\n",
    "depth = data.variables['depth'].values[dep_ini::dep_inc]\n",
    "\n",
    "dep_dim = vpv.shape[0]\n",
    "lat_dim = vpv.shape[1]\n",
    "lon_dim = vpv.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geodetic-geocentric projection\n",
    "LAT, ALT, LON = np.meshgrid(latitude, -1e3*depth, longitude)\n",
    "x, y, z = pm.geodetic2ecef(LAT, LON, ALT)\n",
    "_, DEP, _ = np.meshgrid(latitude, depth, longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-receivers options\n",
    "rec_typ = 'ISC_array'\n",
    "\n",
    "# saving parameters\n",
    "num_pts = x.size\n",
    "num_epo = int(2001)\n",
    "num_blo = 21 #20\n",
    "coo_sys = 'cartesian'\n",
    "vel_sha = 'sphere'\n",
    "vel_typ = 'gladm25'\n",
    "num_neu = 512\n",
    "lea_rat = 5e-6\n",
    "act_fun = torch.nn.ELU\n",
    "bat_siz = 512 #num_pts // 100\n",
    "ada_wei = False\n",
    "opt_fun = torch.optim.Adam\n",
    "dev_typ = \"cuda\"\n",
    "bac_vel = 10 #6 #10.5\n",
    "hyp_par = (\n",
    "    str(num_epo) + '_' +\n",
    "    str(num_blo) + '_' +\n",
    "    str(lea_rat) + '_' +\n",
    "    str(dep_dim) + '_' +\n",
    "    str(lat_dim) + '_' +\n",
    "    str(lon_dim) + '_' +\n",
    "    str(act_fun) + '_' +\n",
    "    str(ada_wei) + '_' +\n",
    "    str(num_pts) + '_' +\n",
    "    str(bat_siz) + '_' +\n",
    "    vel_sha + '_' +\n",
    "    vel_typ + '_' +\n",
    "    str(num_neu) + '_' +\n",
    "    coo_sys + '_' +\n",
    "    str(opt_fun) + '_' +\n",
    "    str(bac_vel) + '_' +\n",
    "    rec_typ + '_' +\n",
    "    dev_typ\n",
    ")\n",
    "\n",
    "# path\n",
    "model_path = \"./../models/\" + hyp_par\n",
    "figures_path = model_path + '/'\n",
    "checkpoints_path = figures_path + 'checkpoints' + '/'\n",
    "predictions_path = figures_path + 'predictions' + '/'\n",
    "\n",
    "Path(figures_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(checkpoints_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(predictions_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Points: Multi-source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISC_array\n"
     ]
    }
   ],
   "source": [
    "# laod all stations\n",
    "ISCall = pd.read_csv('stations.csv')\n",
    "ISCall = ISCall.rename(columns={\"X\":\"LON\", 'Y':'LAT'})\n",
    "\n",
    "# laod only active stations (from ISC website) to 2021\n",
    "ISCarray = ISCall[ISCall['description'].str.contains('to 2021')]\n",
    "ISCarrLon = ISCarray['LON']\n",
    "ISCarrLat = ISCarray['LAT']\n",
    "\n",
    "# load US array data\n",
    "USarray = pd.read_excel('_US-TA-StationList.xls')\n",
    "USarrLon = USarray['LON']\n",
    "USarrLat = USarray['LAT']\n",
    "\n",
    "# concatenate the two receiver group\n",
    "AllLon = np.hstack((USarrLon, ISCarrLon))\n",
    "AllLat = np.hstack((USarrLat, ISCarrLat))\n",
    "\n",
    "# model specifications\n",
    "ear_rad = 6371\n",
    "\n",
    "# source vectors\n",
    "if rec_typ == 'ISC_array':\n",
    "    print(rec_typ)\n",
    "    lat_sou = np.array([latitude.flat[np.abs(latitude - i).argmin()] for i in ISCarrLat])\n",
    "    lon_sou = np.array([longitude.flat[np.abs(longitude - i).argmin()] for i in ISCarrLon])\n",
    "elif rec_typ == 'US_array':\n",
    "    print(rec_typ)\n",
    "    lat_sou = np.array([latitude.flat[np.abs(latitude - i).argmin()] for i in USarrLat])\n",
    "    lon_sou = np.array([longitude.flat[np.abs(longitude - i).argmin()] for i in USarrLon])\n",
    "dep_sou = depth.flat[np.abs(depth - 0).argmin()]\n",
    "\n",
    "xx = (ear_rad - DEP) * np.sin(np.radians(LAT+90)) * np.cos(np.radians(180+LON))/(1e3)\n",
    "yy = (ear_rad - DEP) * np.sin(np.radians(LAT+90)) * np.sin(np.radians(180+LON))/(1e3)\n",
    "zz = DEP * np.cos(np.radians(LAT+90)) / (1e3)\n",
    "\n",
    "xx_s = (ear_rad - dep_sou) * np.sin(np.radians(lat_sou+90)) * np.cos(np.radians(180+lon_sou))/(1e3)\n",
    "yy_s = (ear_rad - dep_sou) * np.sin(np.radians(lat_sou+90)) * np.sin(np.radians(180+lon_sou))/(1e3)\n",
    "\n",
    "# coordinates setup\n",
    "sx, sy, sz = pm.geodetic2ecef(lat_sou, lon_sou, -1e3*dep_sou)\n",
    "\n",
    "# rescale\n",
    "X,Y,Z = x/(ear_rad*1e3), y/(ear_rad*1e3), z/(ear_rad*1e3)\n",
    "sx, sy, sz = sx/(ear_rad*1e3), sy/(ear_rad*1e3), sz/(ear_rad*1e3)\n",
    "\n",
    "# # finding source indices using Dask\n",
    "# import dask.array as da\n",
    "# sids = da.where((da.isclose(da.from_array(X.reshape(-1,1), chunks=X.size//1000), da.from_array(sx), atol=1e-16)) & (da.isclose(da.from_array(Y.reshape(-1,1), chunks=X.size//1000), da.from_array(sy), atol=1e-16)) & (da.isclose(da.from_array(Z.reshape(-1,1), chunks=X.size//1000), da.from_array(sz), atol=1e-16)))[0]\n",
    "# sx, sy, sz = X.reshape(-1,1)[sids], Y.reshape(-1,1)[sids], Z.reshape(-1,1)[sids]\n",
    "# sids = sids[sids.astype(bool)].astype(int)\n",
    "# sids = sids.compute()\n",
    "\n",
    "# for plotting only\n",
    "x_plot,y_plot,z_plot = x,y,z\n",
    "\n",
    "# define receiver coordinates\n",
    "xR, yR, zR = X.reshape(-1,1), Y.reshape(-1,1), Z.reshape(-1,1)\n",
    "\n",
    "# define source coordinates\n",
    "xS, yS, zS = np.tile(sx,(X.size//sx.shape[0]+1, 1))[:X.size].reshape(-1,1), np.tile(sy,(Y.size//sy.shape[0]+1,1))[:Y.size].reshape(-1,1), np.tile(sz,(Z.size//sz.shape[0]+1,1))[:Z.size].reshape(-1,1)\n",
    "\n",
    "# define all inputs and output\n",
    "# Xa = np.hstack((xS, yS, zS, xR, yR, zR))\n",
    "Xa = np.hstack((xR, yR, zR, xR, yR, zR))\n",
    "ya = vpv.reshape(-1,1)\n",
    "\n",
    "# permutate\n",
    "perm_idx = torch.randperm(X.size).numpy()\n",
    "Xa[:, :3] = Xa[perm_idx, :3]\n",
    "\n",
    "# input for database\n",
    "Xb = np.copy(Xa)\n",
    "yb = np.copy(ya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4840726, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy to hdf5\n",
    "predictions = [\n",
    "    dep_sou, lat_sou, lon_sou, X, Y, Z, sx, sy, sz, Xa, ya, Xb, yb, sids, x_plot, y_plot, z_plot\n",
    "]\n",
    "name = [\n",
    "    'dep_sou', 'lat_sou', 'lon_sou', 'X', 'Y', 'Z', 'sx', 'sy', 'sz', 'Xa', 'ya', 'Xb', 'yb', 'sids', 'x_plot', 'y_plot', 'z_plot'\n",
    "]\n",
    "for i in range(len(predictions)):\n",
    "    h5f = h5py.File(predictions_path + name[i]+'.h5', 'w')\n",
    "    h5f.create_dataset(name[i], data=predictions[i])\n",
    "    h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Points: Single Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model specifications\n",
    "ear_rad = 6371\n",
    "lat_sou = latitude.flat[np.abs(latitude - 18).argmin()]\n",
    "lon_sou = longitude.flat[np.abs(longitude - -102).argmin()]\n",
    "dep_sou = depth.flat[np.abs(depth - 475).argmin()]\n",
    "\n",
    "xx = (ear_rad - DEP) * np.sin(np.radians(LAT+90)) * np.cos(np.radians(180+LON))/(1e3)\n",
    "yy = (ear_rad - DEP) * np.sin(np.radians(LAT+90)) * np.sin(np.radians(180+LON))/(1e3)\n",
    "zz = DEP * np.cos(np.radians(LAT+90)) / (1e3)\n",
    "\n",
    "xx_s = (ear_rad - dep_sou) * np.sin(np.radians(lat_sou+90)) * np.cos(np.radians(180+lon_sou))/(1e3)\n",
    "yy_s = (ear_rad - dep_sou) * np.sin(np.radians(lat_sou+90)) * np.sin(np.radians(180+lon_sou))/(1e3)\n",
    "\n",
    "# coordinates setup\n",
    "sx, sy, sz = pm.geodetic2ecef(lat_sou, lon_sou, -1e3*dep_sou)\n",
    "\n",
    "# rescale\n",
    "X,Y,Z = x/(ear_rad*1e3), y/(ear_rad*1e3), z/(ear_rad*1e3)\n",
    "sx, sy, sz = sx/(ear_rad*1e3), sy/(ear_rad*1e3), sz/(ear_rad*1e3)\n",
    "\n",
    "# finding source indices\n",
    "sids = np.where((np.isclose(X.reshape(-1,1), sx, atol=1e-16)) & (np.isclose(Y.reshape(-1,1), sy, atol=1e-16)) & (np.isclose(Z.reshape(-1,1), sz, atol=1e-16)))[0]\n",
    "sx, sy, sz = X.reshape(-1,1)[sids], Y.reshape(-1,1)[sids], Z.reshape(-1,1)[sids]\n",
    "sids = sids[sids.astype(bool)].astype(int)\n",
    "\n",
    "# for plotting only\n",
    "x_plot,y_plot,z_plot = x,y,z\n",
    "\n",
    "# define receiver coordinates\n",
    "xR, yR, zR = X.reshape(-1,1), Y.reshape(-1,1), Z.reshape(-1,1)\n",
    "\n",
    "# define source coordinates\n",
    "xS, yS, zS = np.tile(sx,(X.size//sx.shape[0]+1, 1))[:X.size].reshape(-1,1), np.tile(sy,(Y.size//sy.shape[0]+1,1))[:Y.size].reshape(-1,1), np.tile(sz,(Z.size//sz.shape[0]+1,1))[:Z.size].reshape(-1,1)\n",
    "\n",
    "# define all inputs and output\n",
    "Xa = np.hstack((xS, yS, zS, xR, yR, zR))\n",
    "ya = vpv.reshape(-1,1)\n",
    "\n",
    "# input for database\n",
    "Xb = np.copy(Xa)\n",
    "yb = np.copy(ya)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class to_torch(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        # Creating identical pairs\n",
    "        self.data    = Variable(Tensor(data))\n",
    "        self.target  = Variable(Tensor(target))\n",
    "\n",
    "    def send_device(self,device):\n",
    "        self.data    = self.data.to(device)\n",
    "        self.target  = self.target.to(device)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        return x, y, index\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "database = to_torch(Xb[np.logical_not(np.isnan(yb))[:,0]], yb[np.logical_not(np.isnan(yb))[:,0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if offline and old:\n",
    "    nFeatures = 32\n",
    "else:\n",
    "    nFeatures = 6\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, nl=10, activation=act_fun):\n",
    "            super(NN, self).__init__()\n",
    "            self.act = activation()\n",
    "\n",
    "            # Normalization Layer\n",
    "            self.bn0 = torch.nn.BatchNorm1d(num_features=nFeatures, affine=False)\n",
    "\n",
    "            # Input Structure\n",
    "            self.fc0  = Linear(2*3,nFeatures)\n",
    "            self.fc1  = Linear(nFeatures,512)\n",
    "\n",
    "            # Resnet Block\n",
    "            self.rn_fc1 = torch.nn.ModuleList([Linear(512, 512) for i in range(nl)])\n",
    "            self.rn_fc2 = torch.nn.ModuleList([Linear(512, 512) for i in range(nl)])\n",
    "            self.rn_fc3 = torch.nn.ModuleList([Linear(512, 512) for i in range(nl)])\n",
    "\n",
    "            # Output structure\n",
    "            self.fc8  = Linear(512,nFeatures)\n",
    "            self.fc9  = Linear(nFeatures,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        if offline and old:\n",
    "            x   = self.act(self.bn0(self.fc0(x)))\n",
    "        else:\n",
    "            x   = self.act(self.fc0(x))\n",
    "        x   = self.act(self.fc1(x))\n",
    "        for ii in range(len(self.rn_fc1)):\n",
    "            x0 = x\n",
    "            x  = self.act(self.rn_fc1[ii](x))\n",
    "            x  = self.act(self.rn_fc3[ii](x)+self.rn_fc2[ii](x0))\n",
    "\n",
    "        x     = self.act(self.fc8(x))\n",
    "        tau   = abs(self.fc9(x))\n",
    "        return tau\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, ModelPath, device='cuda'):\n",
    "        \n",
    "        self.Params                                          = {}\n",
    "        self.Params['ModelPath']                             = model_path\n",
    "        self.Params['Device']                                = device\n",
    "        self.Params['Pytorch Amp (bool)']                    = False\n",
    "        self.Params['Network']                               = {}\n",
    "        self.Params['Network']['Number of Residual Blocks']  = num_blo\n",
    "        self.Params['Network']['Layer activation']           = act_fun\n",
    "        self.Params['Training']                              = {}\n",
    "        self.Params['Training']['Number of sample points']   = 1e4\n",
    "        self.Params['Training']['Batch Size']                = bat_siz\n",
    "        self.Params['Training']['Validation Percentage']     = 10\n",
    "        self.Params['Training']['Number of Epochs']          = num_epo\n",
    "        self.Params['Training']['Resampling Bounds']         = [0.1,0.9]\n",
    "        self.Params['Training']['Print Every * Epoch']       = 1\n",
    "        self.Params['Training']['Save Every * Epoch']        = 10\n",
    "        self.Params['Training']['Learning Rate']             = lea_rat\n",
    "        self.Params['Training']['Use Scheduler (bool)']      = True\n",
    "\n",
    "        # Parameters to alter during training\n",
    "        self.total_train_loss = []\n",
    "        self.total_val_loss   = []\n",
    "\n",
    "    def EikonalLoss(self,Yobs,Xp,tau,device):\n",
    "        dtau  = torch.autograd.grad(\n",
    "            outputs=tau, \n",
    "            inputs=Xp, \n",
    "            grad_outputs=torch.ones(tau.size()).to(device), \n",
    "            only_inputs=True,\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "        T0    = torch.sqrt(((Xp[:,3]-Xp[:,0])**2 + (Xp[:,4]-Xp[:,1])**2 + (Xp[:,5]-Xp[:,2])**2)) \n",
    "        T1    = (T0**2)*(dtau[:,3]**2 + dtau[:,4]**2 + dtau[:,5]**2)\n",
    "        T2    = 2*tau[:,0]*(dtau[:,3]*(Xp[:,3]-Xp[:,0]) + dtau[:,4]*(Xp[:,4]-Xp[:,1]) + dtau[:,5]*(Xp[:,5]-Xp[:,2]))\n",
    "        T3    = tau[:,0]**2\n",
    "        if bac_vel:\n",
    "            S2 = (T1+T2+T3)/(bac_vel**2)\n",
    "        else:\n",
    "            S2 = (T1+T2+T3)\n",
    "        if (S2==0).any():\n",
    "            print(\"Whoops!\")\n",
    "        Ypred = torch.sqrt(1/S2)\n",
    "        diff  = abs(Yobs[:,0]-Ypred)/Yobs[:,0]\n",
    "\n",
    "        src_loc = torch.from_numpy(np.array((sx, sy, sz, sx, sy, sz)).reshape(-1,6)).to(torch.device(dev_typ)).float()\n",
    "        tau_src = self.network(src_loc)\n",
    "        vel_src = torch.from_numpy(1/np.array(yb[sids].mean()).reshape(-1)).to(torch.device(dev_typ)).float()\n",
    "        # vel_src = torch.from_numpy(1/yb[sids]).to(torch.device(dev_typ)).float()\n",
    "        loss  = torch.mean(abs((Yobs[:,0]-Ypred)/Yobs[:,0])) + torch.sum(torch.abs(tau_src - vel_src)/vel_src)\n",
    "        \n",
    "        return loss, diff\n",
    "\n",
    "    def init(self):\n",
    "        self.network = NN(nl=self.Params['Network']['Number of Residual Blocks'],activation=self.Params['Network']['Layer activation'])\n",
    "        self.network.apply(init_weights)\n",
    "        self.network.float()\n",
    "        self.network.to(torch.device(self.Params['Device']))\n",
    "\n",
    "    def normalization(self,Xp=None,Yp=None):\n",
    "\n",
    "        xmin_UTM = np.array(copy.copy([np.nanmin(Xb[:,3]),np.nanmin(Xb[:,4]),np.nanmin(Xb[:,5])]))\n",
    "        xmax_UTM = np.array(copy.copy([np.nanmax(Xb[:,3]),np.nanmax(Xb[:,4]),np.nanmax(Xb[:,5])]))\n",
    "\n",
    "        indx = np.argmax(xmax_UTM-xmin_UTM)\n",
    "        self.nf_max    = xmax_UTM[indx]\n",
    "        self.nf_min    = xmin_UTM[indx]\n",
    "        self.sf        = (self.nf_max-self.nf_min)\n",
    "\n",
    "        if (type(Xp)!=type(None)) and (type(Yp)==type(None)):\n",
    "            Xp  = Xp/self.sf\n",
    "            return Xp\n",
    "        if (type(Xp)==type(None)) and (type(Yp)!=type(None)):\n",
    "            Yp  = Yp*self.sf\n",
    "            return Yp\n",
    "        else:\n",
    "            Xp = Xp/self.sf\n",
    "            Yp = Yp/self.sf\n",
    "            return Xp,Yp\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # Initialising the network\n",
    "        self.init()\n",
    "\n",
    "        # Defining the optimization scheme\n",
    "        self.optimizer  = opt_fun(self.network.parameters(),lr=self.Params['Training']['Learning Rate'])\n",
    "        if self.Params['Training']['Use Scheduler (bool)'] == True:\n",
    "            self.scheduler  = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer)\n",
    "\n",
    "        # Creating a sampling dataset\n",
    "        self.dataset = database\n",
    "        self.dataset.send_device(torch.device(self.Params['Device']))\n",
    "        \n",
    "        self.dataset.data,self.dataset.target = self.normalization(Xp=self.dataset.data,Yp=self.dataset.target)\n",
    "\n",
    "        len_dataset         = len(self.dataset)\n",
    "        n_batches           = int(len(self.dataset)/int(self.Params['Training']['Batch Size']) + 1)\n",
    "        training_start_time = time.time()\n",
    "\n",
    "        # Splitting the dataset into training and validation\n",
    "        indices            = list(range(int(len_dataset)))\n",
    "        validation_idx     = np.random.choice(\n",
    "            indices, \n",
    "            size=int(len_dataset*(self.Params['Training']['Validation Percentage']/100)), \n",
    "            replace=False\n",
    "        )\n",
    "        train_idx          = list(set(indices) - set(validation_idx))\n",
    "        validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "        train_sampler      = SubsetRandomSampler(train_idx)\n",
    "\n",
    "        train_loader       = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=int(self.Params['Training']['Batch Size'] ),\n",
    "            sampler=train_sampler,\n",
    "            )    \n",
    "        validation_loader  = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=int(self.Params['Training']['Batch Size'] ),\n",
    "            sampler=validation_sampler,\n",
    "        )    \n",
    "\n",
    "        # defining the initial weights to sample by\n",
    "        weights = Tensor(torch.ones(len(self.dataset))).to(torch.device(self.Params['Device']))\n",
    "        weights[validation_idx] = 0.0\n",
    "        print(weights.device)\n",
    "\n",
    "        for epoch in range(1,self.Params['Training']['Number of Epochs']+1):\n",
    "            print_every           = 1\n",
    "            start_time            = time.time()\n",
    "            running_sample_count  = 0\n",
    "            total_train_loss      = 0\n",
    "            total_val_loss        = 0\n",
    "\n",
    "            # defining the weighting of the samples\n",
    "            weights                 = torch.clamp(\n",
    "                weights/weights.max(),\n",
    "                self.Params['Training']['Resampling Bounds'][0],\n",
    "                self.Params['Training']['Resampling Bounds'][1]\n",
    "            )\n",
    "            weights[validation_idx] = 0.0\n",
    "            train_sampler_wei       = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "            train_loader_wei        = torch.utils.data.DataLoader(\n",
    "                                        self.dataset,\n",
    "                                        batch_size=int(self.Params['Training']['Batch Size'] ),\n",
    "                                        sampler=train_sampler_wei,\n",
    "                                      )\n",
    "            weights                 = Tensor(torch.zeros(len(self.dataset))).to(torch.device(self.Params['Device']))\n",
    "\n",
    "            for i, data in enumerate(train_loader_wei, 0):\n",
    "                \n",
    "                # Get inputs/outputs and wrap in variable object\n",
    "                inputs, labels, indexbatch = data\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.float()\n",
    "\n",
    "                if epoch%10==0:\n",
    "                    perm_idx = torch.randperm(inputs.shape[0])\n",
    "                    inputs[:, :3] = inputs[perm_idx, :3]\n",
    "                    # labels = labels[perm_idx]\n",
    "\n",
    "                inputs.requires_grad_()\n",
    "\n",
    "\n",
    "                if self.Params['Pytorch Amp (bool)']:\n",
    "                    with autocast():\n",
    "                        outputs = self.network(inputs)\n",
    "                        loss_value, wv  = self.EikonalLoss(labels,inputs,outputs,torch.device(self.Params['Device']))\n",
    "                else:\n",
    "                    outputs = self.network(inputs)\n",
    "                    loss_value, wv  = self.EikonalLoss(labels,inputs,outputs,torch.device(self.Params['Device']))\n",
    "\n",
    "\n",
    "                loss_value.backward()\n",
    "\n",
    "                # Update parameters\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Updating the weights\n",
    "                weights[indexbatch] = wv\n",
    "\n",
    "                total_train_loss += loss_value.item()\n",
    "                del inputs, labels, indexbatch, outputs, loss_value, wv\n",
    "\n",
    "\n",
    "            # Determining the Training Loss\n",
    "            for i, data_val in enumerate(validation_loader, 0):\n",
    "                inputs_val, labels_val, indexbatch_val = data_val\n",
    "                inputs_val = inputs_val.float()\n",
    "                labels_val = labels_val.float()\n",
    "                inputs_val.requires_grad_()\n",
    "\n",
    "                if self.Params['Pytorch Amp (bool)']:\n",
    "                    with autocast():\n",
    "                        outputs_val = self.network(inputs_val)\n",
    "                        val_loss,wv = self.EikonalLoss(labels_val,inputs_val,outputs_val,torch.device(self.Params['Device']))\n",
    "                else:\n",
    "                    outputs_val  = self.network(inputs_val)\n",
    "                    val_loss,wv  = self.EikonalLoss(labels_val,inputs_val,outputs_val,torch.device(self.Params['Device']))\n",
    "\n",
    "                total_val_loss             += val_loss.item()\n",
    "                del inputs_val, labels_val, indexbatch_val, outputs_val, val_loss, wv\n",
    "\n",
    "\n",
    "            # Creating a running loss for both training and validation data\n",
    "            total_val_loss   /= len(validation_loader)\n",
    "            total_train_loss /= len(train_loader)\n",
    "            self.total_train_loss.append(total_train_loss)\n",
    "            self.total_val_loss.append(total_val_loss)\n",
    "\n",
    "            if self.Params['Training']['Use Scheduler (bool)'] == True:\n",
    "                self.scheduler.step(total_val_loss)\n",
    "\n",
    "            del train_loader_wei,train_sampler_wei\n",
    "\n",
    "            if epoch % self.Params['Training']['Print Every * Epoch'] == 0:\n",
    "                with torch.no_grad():\n",
    "                    print(\"Epoch = {} -- Training loss = {:.4e} -- Validation loss = {:.4e}\".format(epoch,total_train_loss,total_val_loss))\n",
    "\n",
    "            if (epoch % self.Params['Training']['Save Every * Epoch'] == 0) or (epoch == self.Params['Training']['Number of Epochs'] ) or (epoch == 1):\n",
    "                with torch.no_grad():\n",
    "                    self.save(epoch=epoch,val_loss=total_val_loss)\n",
    "\n",
    "    def save(self,epoch='',val_loss=''):\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch'                 : epoch,\n",
    "                'model_state_dict'      : self.network.state_dict(),\n",
    "                'optimizer_state_dict'  : self.optimizer.state_dict(),\n",
    "                'train_loss'            : self.total_train_loss,\n",
    "                'val_loss'              : self.total_val_loss\n",
    "            }, \n",
    "            '{}/Model_Epoch_{}_ValLoss_{}.pt'.format(self.Params['ModelPath'],str(epoch).zfill(5),val_loss)\n",
    "        )\n",
    "\n",
    "    def load(self,filepath):\n",
    "        # -- Loading model information\n",
    "        self.init()\n",
    "        checkpoint            = torch.load(filepath,map_location=torch.device(self.Params['Device']))\n",
    "        self.total_train_loss = checkpoint['train_loss']\n",
    "        self.total_val_loss   = checkpoint['val_loss']\n",
    "        self.network.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.network.to(torch.device(self.Params['Device']))\n",
    "\n",
    "    def traveltimes(self,Xt,projection=True,normalization=True):\n",
    "        \n",
    "\n",
    "        TT_res = torch.empty(Xb.shape[0], device=torch.device(self.Params['Device']))\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for i, Xp in enumerate(Xt, 0):\n",
    "                if i==0:\n",
    "                    eva_bat=Xp.shape[0]\n",
    "                # Apply projection from LatLong to UTM\n",
    "                Xp  = Xp.to(torch.device(self.Params['Device']))\n",
    "                if projection:\n",
    "                    Xp  = self.projection(Xp)\n",
    "                if normalization:\n",
    "                    Xp  = self.normalization(Xp=Xp,Yp=None)\n",
    "                tau = self.network(Xp)\n",
    "                T0  = torch.sqrt(((Xp[:,3]-Xp[:,0])**2 + (Xp[:,4]-Xp[:,1])**2 + (Xp[:,5]-Xp[:,2])**2))\n",
    "                TT  = tau[:,0]*T0\n",
    "\n",
    "                TT_res[i*eva_bat:(i+1)*eva_bat] = TT\n",
    "\n",
    "                del i,Xp,tau,T0,TT\n",
    "        return TT_res\n",
    "\n",
    "    def velocity(self,Xt,projection=True,normalization=True):\n",
    "\n",
    "        V_res = torch.empty(Xb.shape[0], device=torch.device('cpu'))\n",
    "        for i, Xp in enumerate(Xt, 0):\n",
    "            if i==0:\n",
    "                eva_bat=Xp.shape[0]\n",
    "            Xp    = Xp.to(torch.device(self.Params['Device']))\n",
    "            if projection:\n",
    "                Xp  = self.projection(Xp)\n",
    "            if normalization:\n",
    "                Xp  = self.normalization(Xp=Xp,Yp=None)\n",
    "            Xp.requires_grad_()\n",
    "            tau   = self.network(Xp)\n",
    "            dtau  = torch.autograd.grad(\n",
    "                outputs=tau, \n",
    "                inputs=Xp, \n",
    "                grad_outputs=torch.ones(tau.size()).to(torch.device(self.Params['Device'])),\n",
    "                only_inputs=True,\n",
    "                create_graph=True,\n",
    "                retain_graph=True\n",
    "            )[0] \n",
    "            T0    = torch.sqrt(((Xp[:,3]-Xp[:,0])**2 + (Xp[:,4]-Xp[:,1])**2 + (Xp[:,5]-Xp[:,2])**2))  \n",
    "            T1    = (T0**2)*(dtau[:,3]**2 + dtau[:,4]**2 + dtau[:,5]**2)\n",
    "            T2    = 2*tau[:,0]*(dtau[:,3]*(Xp[:,3]-Xp[:,0]) + dtau[:,4]*(Xp[:,4]-Xp[:,1]) + dtau[:,5]*(Xp[:,5]-Xp[:,2]))\n",
    "            T3    = tau[:,0]**2\n",
    "            Ypred = torch.sqrt(1/(T1+T2+T3)).detach()\n",
    "            if normalization:\n",
    "                Ypred = self.normalization(Yp=Ypred)\n",
    "\n",
    "            V_res[i*eva_bat:(i+1)*eva_bat] = Ypred\n",
    "            del Xp,tau,dtau,T0,T1,T2,T3,Ypred\n",
    "            \n",
    "        return V_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(model_path, device=\"cuda:0\")\n",
    "model.load('/home/taufikmh/KAUST/spring_2022/global_pinns/01_clean_implementations/models/pretrained_small_USarray/Model_Epoch_00170_ValLoss_0.006766859855141995.pt')\n",
    "# model.load('/home/taufikmh/KAUST/summer_2021/global_pinns/07_pytorch_implementation/models/bacvel_10_shallow/Model_Epoch_00160_ValLoss_0.003003701814964081.pt')\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/taufikmh/KAUST/spring_2022/global_pinns/01_clean_implementations/models/pretrained_USarr_div3_bl21_n512/\"\n",
    "latest_model = model_path + \"Model_Epoch_00170_ValLoss_0.006766859855141995.pt\"\n",
    "model = Model(model_path,device=torch.device('cpu'))\n",
    "model.load(latest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = torch.utils.data.DataLoader(\n",
    "    torch.from_numpy(Xb).to(torch.float).to(torch.device('cuda')),\n",
    "    batch_size=int(Xb.shape[0])\n",
    ")\n",
    "T_pred = model.traveltimes(Xt, projection=False, normalization=False).cpu().reshape(vel_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d28a8c50980d99a58a412893c2eef7e0fc9e924de74c9e494ef03e23cc25d15"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('thesis-pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
